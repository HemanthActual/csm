"""
Download all required models for CSM to run offline
This script downloads all the models needed to run CSM fully locally without internet.
"""

import os
import torch
import argparse
from huggingface_hub import snapshot_download, hf_hub_download
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("model_downloader")

# Default models to download
DEFAULT_LLM = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # smaller model that works well on 8GB VRAM
ASR_MODEL = "openai/whisper-base"
CSM_MODEL = "sesame/csm-1b"

def parse_args():
    parser = argparse.ArgumentParser(description="Download models for offline use")
    parser.add_argument("--output-dir", type=str, default="./local_models",
                        help="Directory to save the downloaded models")
    parser.add_argument("--llm", type=str, default=DEFAULT_LLM,
                        help=f"LLM model to download (default: {DEFAULT_LLM})")
    parser.add_argument("--asr", type=str, default=ASR_MODEL,
                        help=f"ASR model to download (default: {ASR_MODEL})")
    return parser.parse_args()

def download_models(output_dir, llm_model, asr_model):
    """Download all required models and save them locally"""
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Create subdirectories for each model type
    llm_dir = os.path.join(output_dir, "llm")
    asr_dir = os.path.join(output_dir, "asr")
    csm_dir = os.path.join(output_dir, "csm")
    
    os.makedirs(llm_dir, exist_ok=True)
    os.makedirs(asr_dir, exist_ok=True)
    os.makedirs(csm_dir, exist_ok=True)
    
    # Download the models
    logger.info(f"Downloading LLM model: {llm_model}")
    llm_path = snapshot_download(repo_id=llm_model, local_dir=os.path.join(llm_dir, llm_model.split('/')[-1]))
    logger.info(f"LLM model downloaded to: {llm_path}")
    
    logger.info(f"Downloading ASR model: {asr_model}")
    asr_path = snapshot_download(repo_id=asr_model, local_dir=os.path.join(asr_dir, asr_model.split('/')[-1]))
    logger.info(f"ASR model downloaded to: {asr_path}")
    
    logger.info(f"Downloading CSM model: {CSM_MODEL}")
    csm_checkpoint = hf_hub_download(repo_id=CSM_MODEL, filename="ckpt.pt", 
                                     local_dir=os.path.join(csm_dir, CSM_MODEL.split('/')[-1]))
    logger.info(f"CSM model downloaded to: {csm_checkpoint}")
    
    # Download Mimi tokenizer used by CSM
    from moshi.models import loaders
    logger.info("Downloading Mimi tokenizer")
    mimi_weight = hf_hub_download(
        repo_id=loaders.DEFAULT_REPO, 
        filename=loaders.MIMI_NAME,
        local_dir=os.path.join(output_dir, "mimi")
    )
    logger.info(f"Mimi tokenizer downloaded to: {mimi_weight}")
    
    # Return the paths for configuration
    return {
        "llm": llm_path,
        "asr": asr_path,
        "csm": csm_checkpoint,
        "mimi": mimi_weight
    }

def create_config_file(model_paths, output_dir):
    """Create a configuration file with the model paths"""
    config_file = os.path.join(output_dir, "local_config.py")
    
    config_content = f"""
# Local model configuration for CSM
# This file was auto-generated by download_models.py

# Model paths for offline use
MODELS = {{
    "llm": "{model_paths['llm']}",
    "asr": "{model_paths['asr']}",
    "csm": "{model_paths['csm']}",
    "mimi": "{model_paths['mimi']}"
}}
"""
    
    with open(config_file, "w") as f:
        f.write(config_content)
    
    logger.info(f"Configuration file created at: {config_file}")
    return config_file

def main():
    args = parse_args()
    
    logger.info(f"Starting model download process...")
    logger.info(f"Models will be saved to: {args.output_dir}")
    
    # Download models
    model_paths = download_models(args.output_dir, args.llm, args.asr)
    
    # Create configuration file
    config_file = create_config_file(model_paths, args.output_dir)
    
    logger.info("Model download complete! The system is now ready for offline use.")
    logger.info(f"Run: python modify_for_offline.py to update the code to use local models")
    
if __name__ == "__main__":
    main()
